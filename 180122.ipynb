{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done\n",
    "=====\n",
    "1. Generate dataset instead of reading it in.\n",
    "1. Re-evaluate toolbox of methods after inlining. Consider doing more detailed analysis of why it changes in performance.\n",
    "    1. Note that with inlining turned on, the differences are quite small, however, some of the optimizations have larger code size improvements.\n",
    "    2. Detail what methods were used and how they impacted performance. Compiler explorer will help.\n",
    "        1. Binary search.\n",
    "        2. Interpolation search.\n",
    "3. Measure performance of each improvement.\n",
    "3. Measure which dataset attributes predict performance of interpolate-linear search.\n",
    "4. Measure impact of threading on performance across dataset sizes.\n",
    "3. Measure performance of best recursive and linear search versions of interpolation and binary search across dataset sizes and seeds.\n",
    "1. Write report on toolbox of methods. Only measure full selection since we don't know anything about variance of searches.\n",
    "    1. Show performance of each improvement, and detail why the optimization improves performance.\n",
    "    3. Show which dataset attributes predict performance of interpolate-linear search.\n",
    "    4. Show impact of threading on performance across dataset sizes. Across sizes is important because cache thrashing may affect sizes differently, so y axis should be degradation, not time.\n",
    "    3. Show performance of best recursive and linear search versions of interpolation and binary search across dataset sizes and seeds. Consider skipping full multi-threaded measurements for large sizes.\n",
    "8. Increase granularity of b-sz measurements and some re-labeling.\n",
    "\n",
    "TODO\n",
    "=====\n",
    "1. Fill out sections on threads, different sizes.\n",
    "3. Re-label graphs.\n",
    "4. Edit section on optimizations.\n",
    "2. Add an evaluation for i-lin-simd\n",
    "    \n",
    "Failed Optimizations\n",
    "-----------------------\n",
    "1. Immediate return and break were worse. Likely because they no longer generated cmov instr. Important and non-intuitive.\n",
    "1. Divide into several intervals, count the number of comparisons passed, and recurse on remaining sub-interval.\n",
    "2. Use array left, mid, right. Choose left right based on x which is determined by the comparison. Already have branchless.\n",
    "3. Interpolation search followed by exponential search.\n",
    "4. Use derivative of the slope to reduce to multiplication.\n",
    "5. Alternate interpolation with linear search.\n",
    "6. Store linear splines on the array and interpolate on the containing spline.\n",
    "7. Do two interpolations and linear search.\n",
    "8. Use interger division for interpolation.\n",
    "9. Test for equality first.\n",
    "10. Guard linear search by indexes.\n",
    "11. Use new shift on every iteration.\n",
    "\n",
    "Subsets Study\n",
    "--------\n",
    "1. Break measurements into subsets and measure differences across sizes. Find minimal subset size. May vary across array sizes.\n",
    "2. Vary seed for subset choice as well as dataset generation.\n",
    "3. How similar are searches for different subsets? Subset variation predicts point query variation.\n",
    "4. How much impact does branch prediction make? Subset search in sub-cycles.\n",
    "\n",
    "Large Array Improvements\n",
    "-------------\n",
    "1. Improve linear search with gather + skip strategies.\n",
    "3. Plot against size, selection, threads on larger arrays.\n",
    "8. Improve search by incorporating binary search on the error.\n",
    "9. How many datasets needed to be representative?\n",
    "\n",
    "Results\n",
    "=====\n",
    "* Why is this important? Search is well-studied, but interpolation is neglected. In-memory workloads are important, and increasingly so with data visualization platforms becoming interactive.\n",
    "* Consider discussing the case where the key isn't in the array.\n",
    "\n",
    "Key Contributions\n",
    "--------------------\n",
    "1. List of optimizations for binary search.\n",
    "2. List of optimizations for interpolation search.\n",
    "3. Comparison of interpolation search and binary search.\n",
    "    1. Full selection across sizes.\n",
    "    2. Concurrent search / multi-core minimally degrades performance.\n",
    "5. Relationship between mathematical properties of data and performance of interpolation search.\n",
    "    * When to use interpolation search?\n",
    "\n",
    "Notes\n",
    "====\n",
    "* It might also work to require the quotient be no less then one, then do a lookup on the whole division operation by scaling down dividend and the divisor by the log of the divisor and looking up the output.\n",
    "* Another idea for the recursive case is to simply build a table of slopes, and to do a lookup of the appropriate slope for the given interval simply using the midpoint.\n",
    "* Since the recursive interpolation is a win in the large array case, perhaps the number of iterations in the linear search can be parameterized by the size of the array. Indeed, you could stack up a bunch of interpolations and do guard checks and a size check. If you hit the guard check or the size check, then jump to the outro linear search. On small arrays, you add the additional cost of the size check, so some thought should be given to how to avoid that. Maybe a single interpolation should be a special case that always does the linear search.\n",
    "* Using guards was strictly worse than doing a single interpolation for the small array case, but is it possible that it might help the recursive case? Setting code bloat aside, maybe do a single interpolation followed by a short linear search, and then do a recursive interpolation search with guards. Maybe even just do a recursive interpolation search with linear search without explicitly special casing the first one.\n",
    "* SIMD vs Unroll is an unstable choice.\n",
    "* Can use maximum distance information to choose a linear search?\n",
    "* while loop does better until the unrolling. Maybe do runtime unrolling of while loop manually?\n",
    "* I don't think it's needed to do pinning because very little degredation in performance has been measured and pinning should make the benchmarks no worse.\n",
    "* To measure the effect of branch prediction, we discussed altering the subsets search in such a way that each subset would be searched using sub-cycles of length 2k. For k = 2, this would be like (A, B, A, B, C, D, C, D, ...). For simplicity, simply starting at k=1, which simply repeats each query twice.\n",
    "* subsets to get amortization, more subsets to learn about variance of searches. Needed for validity of selection results.\n",
    "* Consider a view where the workset is held constant, but the dataset is increased.\n",
    "* Important to specify that the bars in the performance graphs describe the performance of a single algorithm, with a low amount of error on different datasets.\n",
    "* For designing how to do the subsets, if the order of subset searches is defined, then, within a thread, it's easy to reconstitute which subset a particular measurement belonged to. Since we're focusing on outputing all samples, don't need to worry about sorting output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv,DataFrame,Series,concat\n",
    "from os import popen\n",
    "from subprocess import call,check_output\n",
    "from math import sqrt,log2, floor\n",
    "from matplotlib.pyplot import show, plot, table, subplots, title, legend, gca\n",
    "from scipy.stats import linregress\n",
    "from sklearn.metrics import r2_score,mean_absolute_error\n",
    "from heapq import heappush,heappop\n",
    "from numpy import array,fromiter,int64,histogram\n",
    "from IPython.display import display\n",
    "from itertools import accumulate\n",
    "from collections import Counter\n",
    "import random as rnd\n",
    "n_files = n_samples = 11\n",
    "\n",
    "max_n_thds = 4\n",
    "szs = [10**x for x in range(3,6 +1)]\n",
    "\n",
    "rnd.seed(42)\n",
    "seeds = [rnd.randint(1, 2**63) for _ in range(n_files)]\n",
    "# this is only true for 100% selection\n",
    "runs = [5000] + [10]*8\n",
    "def mode(l): return Counter(l).most_common(1)[0][0]\n",
    "def matrix_norm_vec(m, v): return (v / m.T).T\n",
    "def ip_range(s, q=.25): return (s.quantile(1-q) - s.quantile(q))/s.median()\n",
    "def quantile_p(s, q): return abs((s.quantile(q) - s.median())/s.median())\n",
    "\n",
    "b_lr_algs = 'b-lr b-lr-cond  b-lr-over b-lr-noeq b-lr-for b-lr-noeq-for b-lr-lin'\n",
    "b_sz_algs = 'b-sz b-sz-cond b-sz-noeq b-sz-for b-sz-noeq-for b-sz-pow b-sz-lin'\n",
    "i_algs = 'i i-precompute i-lut i-seq-fp i-seq i-seq-simd'\n",
    "\n",
    "algs = ' '.join([b_lr_algs, b_sz_algs, i_algs])\n",
    "sz_algs = 'i-guard i-seq b-sz-lin'\n",
    "\n",
    "# for margin of error e_m, critical value z, uncertainty sigma, mean mu\n",
    "# n = ((z*sigma)*2/(e_m*mu))^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toolbox of Methods\n",
    "=========\n",
    "1000 elements, 100% selection, all algorithms, many seeds.\n",
    "\n",
    "Structure discussion around a compilation of effectiveness of methods. Make a table of the toolbox of methods. Show impact of each method separately. A --> B --> C\n",
    "\n",
    "The overflow math optimization only makes sense in the context of left + right. I'm not sure that L + R is faster or slower than L + N, but I have found more optimizations for L + N. Less state changes with L + R, so it may have less work, but L + N may allow for more work to be done independently.\n",
    "\n",
    "Individually, independent calculations and the linear search appear to do worse, however, together, they seem to improve.\n",
    "\n",
    "<table>\n",
    "<tr><th>Method</th><th>LR Impact</th><th>Size Impact</th><th>Interpolation Impact</th></tr>\n",
    "<tr><td>Overflow Math.</td><td>Increased ILP.</td><td>N/A</td><td>N/A</td></tr>\n",
    "<tr><td>Don't test equality.</td><td>Remove 1 out of 3 conditional moves and save an increment.</td><td>Didn't try.</td><td>Measured to be worse.</td></tr>\n",
    "<tr><td>Reduce size to power of 2.</td><td>Didn't try.</td><td>Save on subtraction in core loop.</td><td>Didn't try, not likely to stay power of 2.</td></tr>\n",
    "<tr><td>Precompute logarithm and use for loop.</td><td>Run-time loop unrolling.</td><td>Run-time loop unrolling. Combined with no-equality, eliminates all conditional moves in critical path. Only branches, moves, and arithmetic.</td><td>Can't precompute number of iterations, but can simply upper bound them.</td></tr>\n",
    "<tr><td>Linear search.</td><td>Check two per cycle instead of every other cycle.</td><td>Check two per cycle instead of every other cycle.</td><td>Linear search can speculatively execute and check many elements while interpolation search can't.</td></tr>\n",
    "</table>\n",
    "\n",
    "Binary Left + Size\n",
    "---------------------\n",
    "<table>\n",
    "<tr><th>Method</th><th>Size Impact</th></tr>\n",
    "<tr><td>Reduce size to power of two</td><td>Save on the subtraction in core loop.</td></tr>\n",
    "<tr><td>Precompute logarithm and use for loop.</td><td>Run-time loop unrolling.</td></tr>\n",
    "<tr><td>Linear search.</td><td>Check two per cycle instead of every other cycle. Worse at first.</td></tr>\n",
    "</table>\n",
    "\n",
    "Note that linear search version is best with a lower unroll factor relative to recursive version. This makes sense since the default is unroll factor of 8, so the linear search version will never use the unrolled version.\n",
    "\n",
    "Binary Left + Right\n",
    "----------------------\n",
    "<table>\n",
    "<tr><th>Method</th><th>LR Impact</th></tr>\n",
    "<tr><td>Overflow math</td><td>Increased ILP.</td></tr>\n",
    "<tr><td>Don't test equality.</td><td>Remove 1 out of 3 conditional moves and save an increment.</td></tr>\n",
    "<tr><td>Precompute logarithm and use for loop.</td><td>Run-time loop unrolling.</td></tr>\n",
    "<tr><td>Use for loop without testing for equality.</td><td>Eliminates all conditional moves in critical path. Only branches, moves, and arithmetic.</td></tr>\n",
    "<tr><td>Linear search.</td><td>Check two per cycle instead of every other cycle.</td></tr>\n",
    "</table>\n",
    "\n",
    "Note that the for loop LR is testing for equality.\n",
    "\n",
    "Not testing for equality has one effect. Using a for loop has a different effect. In combination, a third effect. How to discuss?\n",
    "\n",
    "Note that the size version already is using the overflow arithmetic, not testing for equality and for loop unrolling.\n",
    "\n",
    "Interpolation\n",
    "---------------\n",
    "An interpolation is defined to be\n",
    "left + (x - A[left]) / (A[right]-A[left]) * (right-left)\n",
    "<table>\n",
    "<tr><th>Method</th><th>Impact</th></tr>\n",
    "<tr><td>Precompute part of the interpolation.</td><td>Saves several operations and type conversions in the most critical path.</td></tr>\n",
    "<tr><td>Linear search + 1 interpolation</td><td>Linear search can speculatively execute and check many elements while interpolation search can't.</td></tr>\n",
    "<tr><td>Lookup Table Division</td><td>Save on conversions at the expense of some accuracy, moves, and register pressure. I expect lower CPI with LUT due to instruction parallelism.</td></tr>\n",
    "<tr><td>Store value in struct rather than array.</td><td>Saves a number of moves.</td></tr>\n",
    "<tr><td>Interpolate from 0 to the max value.</td><td>Saves a subtraction.</td></tr>\n",
    "</table>\n",
    "\n",
    "It's important to respect your code size.\n",
    "* Doing division on floating point calculation of interpolation before the multiplication was a big win. One reason is that floating point division works exactly if numerator = denominator, but loses that after you do the multiplication and lose the precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "call(['make', 'release', 'N_SAMPLES=-1', 'NSORT=1', 'N_RUNS=1000'])\n",
    "sz = 1000\n",
    "n_thds = 1\n",
    "algs_ns = [[i, run] + list(alg_ns)\n",
    "           for i, seed in enumerate(seeds)\n",
    "           for run, alg_ns in read_csv(popen('./search {} {} {} {}'.format(sz, seed, n_thds, algs))).iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "algs_ns_df = DataFrame.from_records(algs_ns, columns=['seed', 'run'] + algs.split()\n",
    "                                   ).set_index(['seed', 'run'])\n",
    "improve_ns = DataFrame([(alg, cycles.median(), quantile_p(cycles, .75),\n",
    "                         quantile_p(cycles, .25))\n",
    "                        for alg, cycles in algs_ns_df.iteritems()]\n",
    "                       , columns=['alg', 'median', 'plus25', 'minus25']\n",
    "                      ).set_index(['alg'])\n",
    "for t_algs in [b_lr_algs, b_sz_algs, i_algs]:\n",
    "    improve_ns['median'].loc[t_algs.split()] = (improve_ns.loc[t_algs.split()[0]\n",
    "                                                              ]['median'] /\n",
    "                                                improve_ns.loc[t_algs.split()]['median'])\n",
    "    for quantile in ['plus25', 'minus25']:\n",
    "        improve_ns[quantile].loc[t_algs.split()] = (improve_ns.loc[t_algs.split()\n",
    "                                                                  ][quantile] *\n",
    "                                                    improve_ns.loc[t_algs.split()\n",
    "                                                                  ]['median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(improve_ns.to_csv('improve.dat', index_label='alg', sep=' '))\n",
    "for name, plot_algs in zip(['b-lr', 'b-sz', 'i'], [b_lr_algs, b_sz_algs, i_algs]):\n",
    "    g = improve_ns.loc[plot_algs.split()]\n",
    "    g.to_csv('{}-improve.dat'.format(name), index_label='alg', sep=' ')\n",
    "    g['median'].plot.bar(yerr=g['plus25'])\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolation Performance Predictors\n",
    "===================\n",
    "\n",
    "1. Run metrics on data.\n",
    "2. Combine metrics with performance data.\n",
    "3. Show predictiveness of metrics.\n",
    "4. Plot most predictive measurements.\n",
    "\n",
    "Currently only looking at 1K size. Consider looking at all sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_ints(seed):\n",
    "    return [int(num) for num\n",
    "            in popen('./search 1000 {} 1'.format(seed)).read().split()]\n",
    "def interpolate(x, n, y_l, y_r):\n",
    "    if n < 1: return 0\n",
    "    return floor((x-y_l) / (y_r - y_l) * (n-1))\n",
    "def interpolate_l(x, l):\n",
    "    return interpolate(x, len(l), l[0], l[-1])\n",
    "\n",
    "def avg(l):\n",
    "    return sum(l)/len(l)\n",
    "def dist(a_l, b_l):\n",
    "    return [abs(a - b) for a,b in zip(a_l, b_l)]\n",
    "def r2(x,y):\n",
    "    m, b, r, _, _ = linregress(x,y)\n",
    "    return r**2\n",
    "# TODO verify this matches definition\n",
    "def smoothness(nums):\n",
    "    adj_dist = [abs(d) for d in dist(nums[1:], nums[:-1])]\n",
    "    return max(adj_dist) / min(adj_dist)\n",
    "\n",
    "call(['make', 'dump'])\n",
    "    \n",
    "seed_metrics = []\n",
    "for seed in seeds:\n",
    "    nums = sorted(read_ints(seed))\n",
    "    guesses = [interpolate_l(x,nums) for x in nums]\n",
    "    d = sorted(dist(range(len(nums)), guesses))\n",
    "    seed_metrics.append((d[-1], d[int(len(d)*.9)], avg(d),\n",
    "                   r2(range(len(nums)), guesses),\n",
    "                   smoothness(nums)))\n",
    "\n",
    "#dist_d = DataFrame(dist_d).set_index('file').sort_values('l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed_metrics_df = DataFrame(seed_metrics,\n",
    "                            columns=['max', '90', 'l1', 'r2', 'smoothness'])\n",
    "seeds_ns = DataFrame({\n",
    "    'seed': Series(seeds),\n",
    "    'ns': algs_ns_df['i-seq'].unstack(0).median()\n",
    "})\n",
    "# require that there be an odd number of seeds so that median is always\n",
    "# in the set.\n",
    "median_nums = sorted(read_ints(\n",
    "    seeds_ns[seeds_ns['ns'] == seeds_ns['ns'].median()]['seed'].iloc[0]))\n",
    "\n",
    "cdf = Series(dist(range(len(median_nums)),\n",
    "           [interpolate_l(x, median_nums) for x in median_nums]))\n",
    "predict = Series([r2(series, seeds_ns['ns'])\n",
    "                  for _, series in seed_metrics_df.iteritems()],\n",
    "                 list(seed_metrics_df))\n",
    "seed_metrics_df['ns'] = seeds_ns['ns']\n",
    "ns_metrics = seed_metrics_df.sort_values(['ns']).reset_index()[['l1', 'ns']]\n",
    "print(ns_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ns_metrics)\n",
    "cdf.hist(cumulative=True).plot()\n",
    "cdf.to_csv('cdf.dat', sep=' ', index=False)\n",
    "show()\n",
    "\n",
    "predict.name = 'r2'\n",
    "predict.index.name='metric'\n",
    "predict.to_csv('predict.dat', sep=' ', header=True)\n",
    "predict.plot.bar()\n",
    "show()\n",
    "\n",
    "ns_metrics.to_csv('metrics.dat', sep=' ', header=True, index_label='dataset')\n",
    "ns_metrics.plot.scatter(x='ns',y='l1')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best for Size and Threads\n",
    "=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "szs_thds_seeds_runs_ns_m = []\n",
    "for sz, n_runs in zip(szs, [1024] + [10]*8):\n",
    "    call(['make', 'release', 'N_SAMPLES=-1', 'NSORT=1', 'N_RUNS={}'.format(n_runs)])\n",
    "    szs_thds_seeds_runs_ns_m.append([\n",
    "        [sz, n_thds, i, run] + list(alg_ns)\n",
    "        for n_thds in range(1, max_n_thds+1)\n",
    "        for i, seed in enumerate(seeds)\n",
    "        for run, alg_ns in read_csv(popen('./search {} {} {} {}'.format(sz, seed, n_thds, sz_algs))).iterrows()\n",
    "    ])\n",
    "szs_thds_seeds_runs_ns_m = [x for l in szs_thds_seeds_runs_ns_m for x in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#szs_thds_seeds_runs_ns = read_csv('szs_thds_seeds_runs_ns.csv')[\n",
    "#    ['sz', 'n_thds', 'seed'] + sz_algs.split()]\n",
    "szs_thds_seeds_runs_ns = DataFrame(szs_thds_seeds_runs_ns_m, columns=(['sz', 'n_thds', 'seed', 'run'] + sz_algs.split()))\n",
    "szs_thds_seeds_runs_ns.to_csv('szs_thds_seeds_runs_ns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "szs_thds_ns_m = []\n",
    "for sz, g1 in szs_thds_seeds_runs_ns.groupby(['sz']):\n",
    "    ns_1 = g1[g1['n_thds'] == 1].median()[sz_algs.split()]\n",
    "    for n_thds, g2 in g1.groupby(['n_thds']):\n",
    "        szs_thds_ns_m.append([sz, n_thds, max([g2[alg].median() - ns_1[alg] for alg in sz_algs.split()] / ns_1)])\n",
    "    #szs_thds_ns_m.append([sz, n_thds] + [g[alg].median() for alg in sz_algs.split()])\n",
    "szs_thds_ns = DataFrame(szs_thds_ns_m, columns=['sz', 'n_thds', 'diff'])\n",
    "szs_seeds_ns = DataFrame([[sz, seed] + [g[alg].median() for alg in sz_algs.split()]\n",
    "                                        for (sz, seed), g in szs_thds_seeds_runs_ns[szs_thds_seeds_runs_ns['n_thds'] == 1\n",
    "                                                                                   ].groupby(['sz', 'seed'])],\n",
    "                         columns=['sz', 'seed'] + sz_algs.split()\n",
    "                        ).set_index(['sz', 'seed']).stack().unstack(0).reorder_levels([1,0]).sort_index()\n",
    "szs_ns = DataFrame([[sz] + [g[alg].median() for alg in sz_algs.split()]\n",
    "                                        for sz, g in szs_thds_seeds_runs_ns[szs_thds_seeds_runs_ns['n_thds'] == 1\n",
    "                                                                                   ].groupby(['sz'])]\n",
    "                        , columns=['sz'] + sz_algs.split()\n",
    "                  ).set_index(['sz'])\n",
    "szs_thds_diff = szs_thds_ns[szs_thds_ns['n_thds'] > 1].set_index(['sz', 'n_thds']).stack().unstack(0) * 100\n",
    "szs_thds_diff.index = szs_thds_diff.index.droplevel(1)\n",
    "\n",
    "norm_szs_ns = matrix_norm_vec(szs_ns, szs_ns['b-sz-lin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "szs_thds_diff.plot.bar()\n",
    "szs_thds_diff.to_csv('szs-thds-diff.dat', sep=' ')\n",
    "\n",
    "szs_ns.to_csv('szs-ns.dat', sep=' ')\n",
    "szs_ns.plot.bar()\n",
    "\n",
    "norm_szs_ns.to_csv('szs-ns-norm.dat', sep=' ')\n",
    "norm_szs_ns.plot.bar()\n",
    "\n",
    "fig, ax = subplots()\n",
    "for (alg, g), clr in zip(szs_seeds_ns.groupby(level=[0]), ['r', 'g', 'y', 'b']):\n",
    "    g.plot.box(color=clr, ax=ax, logy=True)\n",
    "    print(clr, alg)\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
